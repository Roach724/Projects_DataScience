{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from typing import Dict,Text\n",
    "import warnings\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import DeepFM_v2offline as dfm\n",
    "import tensorflow_recommenders as tfrs\n",
    "import seaborn as sns\n",
    "from RFM import RFM\n",
    "import io\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x000002130E71CB88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x00000213169800D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "#max=11560351\n",
    "#deepfm=tf.keras.models.load_model('D:\\\\Github\\\\projects-1\\\\DeepFM\\\\DeepFM')\n",
    "retriever=tf.keras.models.load_model('D:\\\\Github\\\\projects-1\\\\DeepFM\\\\Retrieval')\n",
    "vectorizer=tf.keras.models.load_model('D:\\\\Github\\\\projects-1\\\\DeepFM\\\\Vectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load fundamental log data\n",
    "X=pd.read_csv('D:\\\\Github\\\\dataset\\\\user_log_rs20.csv',usecols=[1,2,3,4,5,6,7])\n",
    "y=X.pop('label').values\n",
    "X=X.to_dict(orient='list')\n",
    "for key in X.keys():\n",
    "    X[key]=np.array(X[key])\n",
    "#vectorizer=dfm.Vectorizer()\n",
    "#vectorizer=dfm.fit_vectorizer(vectorizer,X)\n",
    "#vectorizer.save('Vectorizer')\n",
    "X_vectorize=dfm.vectorize(vectorizer,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve=dfm.Retrieval()\n",
    "tensorboard=tf.keras.callbacks.TensorBoard(log_dir='E:\\\\Tensorboard\\\\DeepFM\\\\',write_graph=True)\n",
    "earlystopping=tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1,patience=3,mode='min',restore_best_weights=True)\n",
    "retrieve.compile(optimizer=tf.keras.optimizers.Adam(0.01),loss=tf.keras.losses.BinaryCrossentropy(),metrics=[tf.keras.metrics.AUC(),tf.keras.metrics.Recall()])\n",
    "retrieve.fit(X_vectorize,y,batch_size=2**14,epochs=10,validation_split=0.3,callbacks=[earlystopping,tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_data=pd.read_csv('D:\\\\Github\\\\dataset\\\\rank_data.csv')\n",
    "user_log=pd.read_csv('D:\\\\Github\\\\dataset\\\\user_log_with_time.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_clicks=user_log.groupby(['user_id'],as_index=False)['item_id'].agg({'item_id':np.size})\n",
    "item_clicks=user_log.groupby(['item_id'],as_index=False)['user_id'].agg({'user_id':np.size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get average embedding vectors of lastN records\n",
    "user_fields=['user_id','user_type','member_type']\n",
    "item_fields=['item_id','item_catalog','item_tag']\n",
    "N=10\n",
    "user_log['time_stamp']=pd.to_datetime(user_log['time_stamp'],dayfirst=True,infer_datetime_format=True)\n",
    "user_sets=user_log.iloc[:100,:].user_id.unique()\n",
    "\n",
    "for user_id in user_sets:\n",
    "    click_history=user_log.loc[user_log['user_id']==user_id,\n",
    "                            ['user_id','item_id','item_tag','item_catalog','time_stamp']][-N:].reset_index(drop=True)\n",
    "    max_record=len(click_history)\n",
    "    click_history['time_weight']=1/(datetime.datetime.now()-click_history['time_stamp']).apply(lambda x: x.days)\n",
    "    time_weight=click_history['time_weight'].values\n",
    "    global_embedding_matrix=[]\n",
    "    for idx in range(max_record):\n",
    "        feature_embedding_matrix=[]\n",
    "        for feature in item_fields:\n",
    "            feature_input=np.array([click_history.loc[idx,feature]])\n",
    "            vectorize=prep.get_layer(feature+'_vectorize')(feature_input)\n",
    "            embedding=retrieval.get_layer(feature+'_embedding')(vectorize).numpy()\n",
    "            if embedding.ndim==3:\n",
    "                embedding=np.array(tf.keras.layers.Flatten()(embedding).numpy())\n",
    "            feature_embedding_matrix.append(embedding)\n",
    "        feature_embedding_matrix=np.concatenate(feature_embedding_matrix,axis=1)\n",
    "        embedding=retrieval.get_layer(index=17)(feature_embedding_matrix).numpy()\n",
    "        global_embedding_matrix.append(embedding.reshape(1,-1))\n",
    "    global_avg_pooling_embedding=np.concatenate(global_embedding_matrix,axis=0)\n",
    "    global_avg_pooling_embedding=np.average(global_avg_pooling_embedding,axis=0,weights=time_weight)\n",
    "    global_avg_pooling_embedding_df=pd.DataFrame(global_avg_pooling_embedding.reshape(1,-1),\n",
    "                                                columns=['last'+str(N)+'clicks_embeeding_V'+str(k+1) for k in range(8)])\n",
    "    global_avg_pooling_embedding_df['user_id']=user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get user/retrieval item embedding\n",
    "user_sets=rank_data.iloc[:100,:].user_id.unique()\n",
    "embedding_dfs=[]\n",
    "for user_id in user_sets:\n",
    "    user_feature=rank_data.loc[rank_data['user_id']==user_id,user_fields].drop_duplicates()\n",
    "    feature_embedding_matrix=[]\n",
    "    for feature in user_fields:\n",
    "        feature_input=user_feature[feature]\n",
    "        vectorize=prep.get_layer(feature+'_vectorize')(feature_input)\n",
    "        embedding=retrieval.get_layer(feature+'_embedding')(vectorize).numpy()\n",
    "        if embedding.ndim==3:\n",
    "            embedding=tf.keras.layers.Flatten()(embedding).numpy()\n",
    "        feature_embedding_matrix.append(embedding)\n",
    "    feature_embedding_matrix=np.concatenate(feature_embedding_matrix,axis=1)\n",
    "    embedding=retrieval.get_layer(index=16)(feature_embedding_matrix).numpy()\n",
    "    embedding_df=pd.DataFrame(embedding.reshape(1,-1),columns=['user_embedding_V'+str(k+1) for k in range(8)])\n",
    "    embedding_df['user_id']=user_id\n",
    "    embedding_dfs.append(embedding_df)\n",
    "user_embedding_df=pd.concat(embedding_dfs,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sets=rank_data.iloc[:100,:].item_id.unique()\n",
    "embedding_dfs=[]\n",
    "for item_id in item_sets:\n",
    "    item_feature=rank_data.loc[rank_data['item_id']==item_id,item_fields].drop_duplicates()\n",
    "    feature_embedding_matrix=[]\n",
    "    for feature in item_fields:\n",
    "        feature_input=np.array([item_feature[feature]])\n",
    "        vectorize=prep.get_layer(feature+'_vectorize')(feature_input)\n",
    "        embedding=retrieval.get_layer(feature+'_embedding')(vectorize).numpy()\n",
    "        if embedding.ndim==3:\n",
    "            embedding=tf.keras.layers.Flatten()(embedding).numpy()\n",
    "        feature_embedding_matrix.append(embedding)\n",
    "    feature_embedding_matrix=np.concatenate(feature_embedding_matrix,axis=1)\n",
    "    embedding=retrieval.get_layer(index=17)(feature_embedding_matrix).numpy()\n",
    "    embedding_df=pd.DataFrame(embedding.reshape(1,-1),columns=['item_embedding_V'+str(k+1) for k in range(8)])\n",
    "    embedding_df['item_id']=item_id\n",
    "    embedding_dfs.append(embedding_df)\n",
    "item_embedding_df=pd.concat(embedding_dfs,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastN=dfm.get_lastNitem_embedding(vectorizer,retriever,user_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embedding=dfm.get_user_embedding(vectorizer,retriever,rank_data)\n",
    "item_embedding=dfm.get_item_embedding(vectorizer,retriever,rank_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation and obtain the optimal ranker model\n",
    "#model=dfm.cross_validation(X,y,epochs=8)\n",
    "#model.save('DeepFM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard and early stopping callbacks\n",
    "#tensorboard=tf.keras.callbacks.TensorBoard(log_dir='E:\\\\Tensorboard\\\\DeepFM\\\\',write_graph=True)\n",
    "#earlystopping=tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1,patience=3,mode='min',restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''get retrieval for all historical user\n",
    "user_log=pd.read_csv('D:\\\\Github\\\\dataset\\\\user_log.csv',usecols=[1,2,3,4,5,6])\n",
    "items=np.load('items.npy',allow_pickle=True)\n",
    "rank_df=[]\n",
    "users=user_log[['user_id','user_type','member_type']].drop_duplicates()\n",
    "t0=datetime.datetime.now()\n",
    "for tup in zip(users['user_id'],users['user_type'],users['member_type']):\n",
    "    user=dict()\n",
    "    user_item=dict()\n",
    "    user.setdefault('user_id',np.array([tup[0]]))\n",
    "    user.setdefault('user_type',np.array([tup[1]]))\n",
    "    user.setdefault('member_type',np.array([tup[2]]))\n",
    "    log=user_log.loc[user_log['user_id']==tup[0],['user_id','item_id','item_catalog']]\n",
    "    user_item.update(user)\n",
    "    user_item.update(items.all())\n",
    "    res=dfm.guess_you_like(retrieval,prep,user_item,json_like=False,topK=36)\n",
    "    res=res.merge(log.drop_duplicates(),on=['user_id','item_id'],how='left')\n",
    "    rank_df.append(res)\n",
    "    \n",
    "rank_df=pd.concat(rank_df,axis=0)\n",
    "rank_train=rank_df.rename(columns={'item_catalog_x':'item_catalog','score':'similarity','item_catalog_y':'label'}).drop(columns='rank')\n",
    "rank_train['label']=rank_train['label'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "t1=datetime.datetime.now()\n",
    "print('retrieve in '+str(t1-t0))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train item2vec model\n",
    "'''\n",
    "X=pd.read_csv('item_feature.csv')\n",
    "X=X.dropna()\n",
    "y=X.pop('label').values\n",
    "X=X.to_dict(orient='list')\n",
    "for key in X.keys():\n",
    "    X[key]=np.array(X[key])\n",
    "model=dfm.Item2vec(X)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.05),loss=tf.keras.losses.BinaryCrossentropy(),metrics=[tf.keras.metrics.Recall(),tf.keras.metrics.AUC(),tf.keras.metrics.Accuracy()])\n",
    "model.fit(X,y,epochs=20,verbose=2)\n",
    "'''\n",
    "#model.save('item2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load item2vec model\n",
    "#calculate item similarity\n",
    "'''\n",
    "item_features=pd.read_csv('item_profile.csv')\n",
    "model=tf.keras.models.load_model('item2vec')\n",
    "writer=pd.ExcelWriter('sim.xlsx')\n",
    "for item in item_features['item_id'].unique():\n",
    "    sim=dfm.get_similar_items(model,item_features,item_id=item)\n",
    "    name=item_features.loc[item_features['item_id']==item,'item_name'].reset_index(drop=True)\n",
    "    item_id=[]\n",
    "    cos=[]\n",
    "    for key,value in sim['item_list'].items():\n",
    "        item_id.append(key)\n",
    "        cos.append(value)\n",
    "    dat=pd.DataFrame({'item_id':item_id,'sim':cos}).merge(item_features,on='item_id',how='inner').loc[:,['item_id','item_name','sim']]\n",
    "    dat.to_excel(excel_writer=writer,sheet_name=name[0],index=False)\n",
    "writer.save()\n",
    "writer.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write item vectors to tsv file for tensorflow projector\n",
    "'''\n",
    "weights=item_model.get_layer('id_embedding_layer').get_weights()[0]\n",
    "vocab=item_model.get_layer('id_vectorize').get_vocabulary()\n",
    "out_v=io.open('vecs.tsv','w',encoding='utf-8')\n",
    "out_m=io.open('meta.tsv','w',encoding='utf-8')\n",
    "for index,word in enumerate(vocab):\n",
    "    if index==0:continue\n",
    "    vec=weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec])+'\\n')\n",
    "    out_m.write(word+'\\n')\n",
    "out_v.close()\n",
    "out_m.close()\n",
    "'''"
   ]
  }
 ]
}